{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering: Document-Term Matrix, TF-IDF using sklearn and Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_docs = ['This movie was about spaceships and aliens.',\n",
    "           'I really enjoyed the movie!',\n",
    "           'Awesome action scenes, but boring characters.',\n",
    "           'The movie was awful! I hate alien films.',\n",
    "           'Space is cool! I like space movies.',\n",
    "           'More space films, please!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vec = CountVectorizer()                      # Instantiate CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer() and TfidfVectorizer() dictionary can be customized as:\n",
    "#   stop_words=stopwords.words('english')      - Drop stopwords from dictionary\n",
    "#   max_df=0.85           - Drop all terms that appear in over 85% of documents\n",
    "#   max_features=10       - Restrict vocabulary to the 10 most frequent terms\n",
    "\n",
    "vec = CountVectorizer(max_df=0.85, stop_words=stopwords.words('english'))\n",
    "vec = CountVectorizer(max_df=0.85, max_features=10, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 5, 'really': 7, 'enjoyed': 1, 'action': 0, 'scenes': 8, 'hate': 3, 'films': 2, 'space': 9, 'like': 4, 'movies': 6}\n"
     ]
    }
   ],
   "source": [
    "vec.fit(my_docs)                             # fit method creates dictionary\n",
    "print(vec.vocabulary_)                       # Display  dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action', 'enjoyed', 'films', 'hate', 'like', 'movie', 'movies', 'really', 'scenes', 'space']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vec.vocabulary_.keys()))        # Sort dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = vec.transform(my_docs)                 # transform() creates count vector\n",
    "dtm = vec.fit_transform(my_docs)             # You can fit and transform jointly\n",
    "dtm.shape                                    # num_docs x dictonary_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 2]\n",
      " [0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()                      # TF-IDF\n",
    "vec = TfidfVectorizer(stop_words='english', max_df=0.85)\n",
    "\n",
    "tfidf = vec.fit_transform(my_docs)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.63509072 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4396812  0.         0.         0.         0.         0.63509072]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.63509072 0.         0.         0.\n",
      "  0.4396812  0.         0.63509072 0.         0.         0.        ]\n",
      " [0.4472136  0.         0.         0.4472136  0.         0.4472136\n",
      "  0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.        ]\n",
      " [0.         0.490779   0.         0.         0.490779   0.\n",
      "  0.         0.         0.         0.4024458  0.490779   0.\n",
      "  0.3397724  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.41923309 0.         0.         0.         0.41923309\n",
      "  0.         0.41923309 0.         0.         0.68755426 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.70710678 0.         0.\n",
      "  0.         0.         0.         0.         0.70710678 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.nnz                                    # nnz number of non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2037037037037037"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.nnz/float(tfidf.shape[0]*tfidf.shape[1]) # Measure of sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(27 unique tokens: ['about', 'aliens', 'and', 'movie', 'spaceships']...)\n"
     ]
    }
   ],
   "source": [
    "tokens = [[w for w in doc.split()] for doc in my_docs]\n",
    "tokens = [simple_preprocess(doc, deacc=True) for doc in my_docs]\n",
    "my_dict = Dictionary(tokens)              # Create a Gensim dictionary\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about': 0, 'aliens': 1, 'and': 2, 'movie': 3, 'spaceships': 4, 'this': 5, 'was': 6, 'enjoyed': 7, 'really': 8, 'the': 9, 'action': 10, 'awesome': 11, 'boring': 12, 'but': 13, 'characters': 14, 'scenes': 15, 'alien': 16, 'awful': 17, 'films': 18, 'hate': 19, 'cool': 20, 'is': 21, 'like': 22, 'movies': 23, 'space': 24, 'more': 25, 'please': 26}\n"
     ]
    }
   ],
   "source": [
    "print(my_dict.token2id)                   # Token-to-id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dict.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['movie', 'spaceship', 'alien'],\n",
       " ['really', 'enjoyed', 'movie'],\n",
       " ['awesome', 'action', 'scene', 'boring', 'character'],\n",
       " ['movie', 'awful', 'hate', 'alien', 'film'],\n",
       " ['space', 'cool', 'like', 'space', 'movie'],\n",
       " ['space', 'film', 'please']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for doc in my_docs:\n",
    "    words = regexp_tokenize(doc.lower(), r'[A-Za-z]+')\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    tokens.append(words)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(17 unique tokens: ['alien', 'movie', 'spaceship', 'enjoyed', 'really']...)\n"
     ]
    }
   ],
   "source": [
    "my_dict = Dictionary(tokens)       \n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alien': 0, 'movie': 1, 'spaceship': 2, 'enjoyed': 3, 'really': 4, 'action': 5, 'awesome': 6, 'boring': 7, 'character': 8, 'scene': 9, 'awful': 10, 'film': 11, 'hate': 12, 'cool': 13, 'like': 14, 'space': 15, 'please': 16}\n"
     ]
    }
   ],
   "source": [
    "print(my_dict.token2id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You can also add new words or lists of words to a dictionary, save a\n",
    "# dictionary to a file, load it back later, or read a dictionary from a text file\n",
    "\n",
    "# my_dict.add_documents(list_of_new_words)\n",
    "# my_dict.save('saved_dict.dict')\n",
    "# loaded_dict = Dictionary.load('saved_dict.dict')\n",
    "# dictionary = Dictionary(line.split()) for line in open('sample.txt', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(0, 1), (1, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(1, 1), (13, 1), (14, 1), (15, 2)],\n",
       " [(11, 1), (15, 1), (16, 1)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = [my_dict.doc2bow(doc) for doc in tokens]\n",
    "dtm                                         # Create Gensim BOW corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alien', 1], ['movie', 1], ['spaceship', 1]]\n",
      "[['movie', 1], ['enjoyed', 1], ['really', 1]]\n",
      "[['action', 1], ['awesome', 1], ['boring', 1], ['character', 1], ['scene', 1]]\n",
      "[['alien', 1], ['movie', 1], ['awful', 1], ['film', 1], ['hate', 1]]\n",
      "[['movie', 1], ['cool', 1], ['like', 1], ['space', 2]]\n",
      "[['film', 1], ['space', 1], ['please', 1]]\n"
     ]
    }
   ],
   "source": [
    "for doc in dtm:\n",
    "    print([[my_dict[i], freq] for i, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alien', 0.51], ['movie', 0.19], ['spaceship', 0.84]]\n",
      "[['movie', 0.16], ['enjoyed', 0.7], ['really', 0.7]]\n",
      "[['action', 0.45], ['awesome', 0.45], ['boring', 0.45], ['character', 0.45], ['scene', 0.45]]\n",
      "[['alien', 0.37], ['movie', 0.14], ['awful', 0.6], ['film', 0.37], ['hate', 0.6]]\n",
      "[['movie', 0.12], ['cool', 0.53], ['like', 0.53], ['space', 0.65]]\n",
      "[['film', 0.46], ['space', 0.46], ['please', 0.76]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "import numpy as np\n",
    "tfidf = TfidfModel(dtm)                     # Compute TF-IDF values\n",
    "for doc in tfidf[dtm]:\n",
    "    print([[my_dict[i], np.around(freq, decimals=2)] for i, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.5132496009471523), (1, 0.1894251567009166), (2, 0.8370740451933879)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[dtm[0]]                              # Sentence weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.3662223291232878),\n",
       " (1, 0.13516176525716664),\n",
       " (10, 0.5972828929894088),\n",
       " (11, 0.3662223291232878),\n",
       " (12, 0.5972828929894088)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[dtm[3]]                              # Rare words weighted higher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
