{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shradha Yadav - U6757772"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Booking.com Reviews\n",
    "\n",
    "##### Questions:\n",
    " This file contains over 515,000 guest reviews and rating of almost 1500 hotels across Europe scraped from popular hotel reservation website Booking.com. The text data was cleaned by removing unicode and punctuation and transformed to lower case. No other preprocessing was done. More information on each field is provided in the \"Data Description\" tab of the Excel file.\n",
    "\n",
    "Extending the analysis of this data set from class, you are asked to answer the following questions:\n",
    "\n",
    "1. What are the top hotel features that guests talk about the most in their Booking.com reviews? Since this is a copy-and-paste from class handout, no points are assigned for this question. (0 point)\n",
    "2. Score each guest review along each of the above features. Note that features that are not mentioned in a given review should receive no score for that review. You may use the approach discussed in class or a different approach of your choice.\n",
    "3. Compute the average feature score for each review, and report the correlation between this average score and the “Reviewer_Score” column in the data file. A strong positive correlation (>0.70) would indicate that your scoring mechanism worked. If your correlation was not up to the mark, you may want to explore what you can do to improve it.\n",
    "4. Compute average feature-wise score for each hotel (across all reviews) and report the correlation between these computed hotel scores and Average_Hotel_Score.\n",
    "5. Create a dashboard with the following plots: (1) Ten highest rated hotels in Europe (showing your aggregate score and Average_Hotel_Score), (2) Ten highest rated hotels for each of the top five features identified in Question 1.\n",
    "6. Regress Average_Hotel_Score against the top five feature scores of each hotel, and provide quantitative estimates of how important each feature is for the Average_Hotel_Score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### steps -,\n",
    "    1. Read questions,\n",
    "    2. Identify columns required to answer - hotel,Average_Hotel_Score, positive_revew, negative_review,Reviewer_Score ,\n",
    "    3. Identify datatypes and if uniform through or not,\n",
    "    4. Null and missing values in required column,\n",
    "    5. no. of records in excel,\n",
    "    6. Functions required -> Feature extraction,polarity/scoring, correlation, regression, plot(multiple graph),\n",
    "    7. Unique column/indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob, Word\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "stopwords = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "\n",
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/shradhayadav/Documents/Text_Analytics/Data/BookingDotCom_HotelReviews.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.droping not required column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Non_Review_Scoring_Count']\n",
    "df.drop(to_drop, inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Changing the index of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.set_index(['col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. check & remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.isnull().values.any():\n",
    "    # get indices\n",
    "    print(df.isnull().any())\n",
    "else:\n",
    "    print('No Null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since Comments has ' No Positive' & 'No Negative' text - which means no input from customer\n",
    "#thus we can remove these\n",
    "df.loc[df['Positive_Review']=='No Positive''Positive_Review'] = ''\n",
    "df['Positive_Review'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Negative_Review']=='No Negative''Negative_Review'] = ''\n",
    "df['Negative_Review'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### drop null if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Positive_Review','Negative_Review'], inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### check and convert datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using sample dataset to runthrough code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.sample(n=60000)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the DataFrame.applymap() function to clean the entire dataset, element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming columns to a more recognizable set of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skipping unnecessary rows in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps,\n",
    "    1. Word Tokenize all comments,\n",
    "    2. Remove stopwords & Puctuations,\n",
    "    3. Lemmatization,\n",
    "    4. Sort nouns by their frequencies,\n",
    "    5. Fetch top 15/10/5 to identify top feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5(d):\n",
    "        comment_list = d.tolist()                                     # Convert a Pandas series to a list\n",
    "        comment_list = [c for c in comment_list if pd.isnull(c)==False]\n",
    "        comment_str  = ' '.join(comment_list)                          # Concatenate all comments to one string\n",
    "        tokenized = nltk.word_tokenize(comment_str)                    # Tokenize into words\n",
    "        tokenized = [w for w in tokenized if len(w)>2]\n",
    "        tagged = nltk.pos_tag(tokenized tagset='universal')           # Extract POS tags for each word\n",
    "        bigrams = nltk.bigrams(tagged)                                 # Extract bigrams of POS tagged words\n",
    "        adj_noun = [(x y) for x y in bigrams if x[1]=='ADJ' and y[1]=='NOUN']\n",
    "        adj_noun = [i[0][0] + ' ' + i[1][0] for i in adj_noun]         # Convert ADJ-NOUN bigram list into a list of strings\n",
    "        return nltk.FreqDist(adj_noun).most_common(5)                  # Return the 5 most common ADJ-NOUN strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_nouns(d):\n",
    "    comment_list = d.tolist()\n",
    "    comment_list = [c for c in comment_list if pd.isnull(c)==False]\n",
    "    comment_str  = ' '.join(comment_list) \n",
    "    words = nltk.word_tokenize(comment_str) \n",
    "    words = [w for w in words if len(w)>2]\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    tagged = nltk.pos_tag(words tagset='universal')\n",
    "    nouns = [w[0] for w in tagged if w[1]=='NOUN']\n",
    "    return nltk.FreqDist(nouns).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since few reviews starts with number converting datatype to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = dfs['Positive_Review'].str.lower()\n",
    "dneg = dfs['Negative_Review'].str.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question-1 . What are the top hotel features that guests talk about the most in their Booking.com reviews? Since this is a copy-and-paste from class handout, no points are assigned for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos_features = top_5(dpos)\n",
    "print(top_pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_Neg_features = top_5(dneg)\n",
    "print(top_Neg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_nouns(dpos)\n",
    "top_10_nouns(dneg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question-1 Analysis/Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top features :\n",
    "\n",
    "From Positive Comments - \n",
    "From Negative Comments -\n",
    "\n",
    "\"Hotel\" is not a feature of a hotel. So we will exclude it from analysis.\n",
    "\n",
    "Combining the two sets of features top features overall that we will be using for scoring are : staff location room breakfast bed bathroom service\n",
    "\n",
    "Note: Random sample (n=60000) gives the same features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question-2. Score each guest review along each of the above features. Note that features that are not mentioned in a given review should receive no score for that review. You may use the approach discussed in class or a different approach of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence tokenize each review \n",
    "    1.Concatenating positive & Negative reviews for each reviewer by period\n",
    "    \n",
    "    2.Updating any word in upper to lower to keep uniform capitalize case\n",
    "    \n",
    "    3.Tokenizing reviews by Capital character after appending period assuming new sentence starting with Capital\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews=[]\n",
    "for i in df.index:\n",
    "    review = df['Positive_Review'][i]\n",
    "    review = str(review).strip() \n",
    "    \n",
    "    if  review.islower() or review.isupper(): \n",
    "        review = review.capitalize()\n",
    "    print(review)\n",
    "\n",
    "#     r_split = review.split()    #some sentences are upper case  capitalizing \n",
    "#     for i in range(len(r_split)):\n",
    "#         if len(r_split[i])> 2 and r_split[i].isupper():\n",
    "#             r_split[i] = r_split[i].lower()\n",
    "\n",
    "#     review = ' '.join(r_split)\n",
    "#     regex = re.sub( r\"([A-Z]\\w+[a-z])\", r\".\\1\", review)\n",
    "    reviews.append(review)\n",
    "# print(reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_cleanup(comment):\n",
    "    reviews=[]\n",
    "    for i in df.index:\n",
    "        review = comment[i]\n",
    "        review = str(review).strip() \n",
    "        if  review.islower() or review.isupper(): \n",
    "            review = review.capitalize()\n",
    "\n",
    "        r_split = review.split()    #some sentences are upper case  capitalizing \n",
    "        for i in range(len(r_split)):\n",
    "            if len(r_split[i])> 2 and r_split[i].isupper():\n",
    "                r_split[i] = r_split[i].lower()\n",
    "\n",
    "        review = ' '.join(r_split)\n",
    "        regex = re.sub( r\"([A-Z]\\w+[a-z])\", r\".\\1\", review)\n",
    "        reviews.append(review)\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pos = df['Positive_Review']\n",
    "P_review = comment_cleanup(df_Pos)\n",
    "print(P_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Neg = df['Negative_Review']\n",
    "N_review = comment_cleanup(df_Neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate Positive & Negative reviews and add to Dataframe df as a column 'Reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews = [i + j for i, j in zip(P_review, N_review)]\n",
    "df['Reviews'] = Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(Reviews):\n",
    "    \n",
    "    regex = re.sub( r\"([A-Z]\\w+[a-z])\", r\".\\1\", Reviews)\n",
    "    merged_review = regex.split('.')\n",
    "   \n",
    "    for m in merged_review:\n",
    "        if m == ' ' or m=='':\n",
    "            merged_review.remove(m)\n",
    "\n",
    "           \n",
    "    for m in merged_review:\n",
    "        reviews.append(m)\n",
    "        for f in features:\n",
    "            if f in m:\n",
    "                polarity_score = []\n",
    "                analyzer = SentimentIntensityAnalyzer()\n",
    "                p = analyzer.polarity_scores(m)\n",
    "                polarity_score = (p['compound'])\n",
    "\n",
    "                # Recording all tokenized senetences, respective features and each sentence polarity \n",
    "                # against each reviewer in dataframe df_Score \n",
    "    \n",
    "                feature_score_temp = pd.DataFrame({'d_Index':[i], 'Hotel_Name':Hotel_Name\n",
    "                                                   ,'Review':review,'Review_sent':m,'Feature':f\n",
    "                                                   ,'Polarity_Score': polarity_score,'Reviewer_Score': Reviewer_Score\n",
    "                                                   ,'Average_Hotel_Score': Average_Hotel_Score\n",
    "                                                  })\n",
    "                df_Score = pd.concat([df_Score,feature_score_temp])\n",
    "\n",
    "\n",
    "                                                                                        \n",
    "df_Score=df_Score.drop(['d_Index'],axis=1)\n",
    "df_Score.reset_index(drop=True, inplace=True) \n",
    "df_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    print(df['Reviews'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to score each review by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mean\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def scoring(Reviews,feature):\n",
    "    sent_list = nltk.sent_tokenize(Reviews)\n",
    "    sent_list = [s for s in sent_list if re.search(feature,s) != None]\n",
    "    polarity = [analyzer.polarity_scores(s)['compound'] for s in sent_list]\n",
    "    if len(polarity) >=1:\n",
    "        mean_polarity = mean(polarity)\n",
    "    else:\n",
    "        mean_polarity =0\n",
    "    \n",
    "    scaled_polarity = round((mean_polarity*10 + 10)/2,1)\n",
    "    return scaled_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "starttime = time.time()\n",
    "\n",
    "features = ['staff', 'location', 'room', 'bed', 'breakfast', 'bathroom']\n",
    "\n",
    "for f in features:\n",
    "    df[f] = pd.Series([scoring(r,f) for r in df['Reviews']])\n",
    "\n",
    "print('This process took', (time.time() - starttime)/60, ' minutes')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
